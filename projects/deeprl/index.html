<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Reinforcement Learning | Ji Sung Ahn</title> <meta name="author" content="Ji Sung Ahn"> <meta name="description" content="Reinforcement learning approach to strategic game behavior"> <meta name="keywords" content="ji-sung-ahn, jisung-ahn, robotics"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jisungahn10.github.io/projects/deeprl/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Ji Sung Ahn</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reinforcement Learning</h1> <p class="post-description">Reinforcement learning approach to strategic game behavior</p> </header> <article> <h1 id="context">Context</h1> <p>The Robotics and Mechanisms Laboratory (RoMeLa) participated in RoboCup, an international robot soccer competition with different leagues and robots in order to promot robotics and artificial intelligence research. We entered the Adult-Sized Humanoid (ASH) League where four adult-sized humanoid robots autonomously play a game of soccer against each other. The autonomous robots are required to sense the environment, make decisions, and act based on the decisions to achieve victory. A camera and an inertial measurement unit (IMU) were used for sensing. The sensor information obtained from the camera and the IMU is proccessed to localize the robot, other robots, and the ball in the environment. The processed information is used to make strategic decisions to win games. I developed a strategic game planner based on the subsumption architecture to make timely and informed decisions. I also decided to try a reinforcement learning approach as well.</p> <h1 id="reinforcement-learning">Reinforcement Learning</h1> <p>While it was simple and straightforward to develop basic behaviors and organize them into an augmented finite state machine to make tactical decisions, the behaviors were crafted based on heuristics, which was time consuming, and human estimation, which is not guaranteed to be optimal. There is also no way to evaluate the different behaviors, requiring even more heuristics to decide on which behaviors to keep and which to remove. Reinforcement learning could be used as an approach to quantitatively evaluate the effectiveness of the emergent behavior. Due to the success of reinforcement learning algorithms in board games, such as Chess and Go, and even video games, such as Super Mario and Starcraft, reinforcement learning was explored as a game strategy module. Further success was found in an application of reinforcement learning to a bipedal robot to perform complex behaviors by synthesizing dynamic motions, showing feasibility on real-world. Thus, reinforcement learning is explored as a method to make tactical decisions for a bipedal soccer robot. To do so, it would need to observe the positions of itself, the ball, and the opponents in Cartesian space, resulting in a eight dimensional observation space.</p> <p>A baseline scenario (Figure 1) was defined and developed in a simulation environment to compare reinforcement learning and finite state machine approaches. The environment was built based on the real field dimensions. The scenario starts with the player starting in a random position on the left side of the field and the ball placed in the center of the field. The scenario ends when the ball is kicked into the opposition’s goal. The time taken to complete the scenario is averaged over multiple trials to measure the performance of each approach. The standard deviation of the time taken over the multiple trials is used to measure the consistency and reliability of each approach. Below is an example of the scenario.</p> <div class="row"> <div class="col-2"> </div> <div class="col-8"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/rl/gif/rl_sample_game.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/rl/gif/rl_sample_game.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/rl/gif/rl_sample_game.gif-1400.webp"></source> <img src="/assets/rl/gif/rl_sample_game.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="sample game" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-2"> </div> </div> <div class="caption"> Figure 1: Baseline Scenario </div> <h2 id="q-learning">Q-Learning</h2> <p>A basic Q-learning algorithm was first used to train a robot in a grid-based world to kick a ball into the goal. The initial attempts at using Q-learning showed promise, but the algorithm quickly ran into the curse of dimensionality. The algorithm managed to reach ending conditions when the observation space was four dimensional (positions of the robot and the ball), but immediately struggled when the grid was expanded from a 9 by 14 table to 18 by 28 table. In other words, the basic Q-learning algorithm using a grid-based world is not scalable. Furthermore, in order to produce quality decisions, it is necessary for the model to train on an environment that accurately represents the real world, so compromising for a simpler environment with fewer discrete spaces is not an option.</p> <h2 id="deep-q-network">Deep Q Network</h2> <p>To deal with the challenges of dimensionality, Deep Q Network (DQN) was also explored. DQN is a model-free network that does not require its environment’s transition functions to make predictions of future states or rewards. Since the planner is correlated to building a planner for the actual soccer game, the RL model-free approach is expected to give a good robust solution. DQN is built on Fitted Q-Iteration, which uses different tricks to stabilize the learning with neural networks. I used stable-baseline3 library, which comes with a set of reliable implementations of Reinforced Learning algorithms. The DQN algorithm in this library has provided us with Vanilla Deep Q learning implementation.</p> <p>The observation space is continuous, but the action space is discrete. A large amount of time was spent building a custom environment with strategically placed rewards to encourage certain behaviors such as getting closer to the ball and kicking the ball towards the opposition’s goal. Negative reward is accrued each time step to avoid making unnecessary repetitive actions. Due to time constraints, the task has been simplified to kicking the ball towards the opposition’s goal without any opponents. The opponents were removed from the observation space because designing collisions into the model was causing the agent to freeze in one position.</p> <p>The final exploration rate influenced how robust and accurate the model was to different types of situations. A low final exploration rate produces a highly constrained action, but is prone to getting stuck in local optimas. A high final exploration rate would require exponentially more epochs, but would be more robust against local optimas. Different models with a final exploration rates of 0.05, 0.3, and 0.5 were trained and the exploration rate (Figure 2) with the highest reward was chosen.</p> <div class="row"> <div class="col-2"> </div> <div class="col-8"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/rl/img/DifferentExpDQN-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/rl/img/DifferentExpDQN-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/rl/img/DifferentExpDQN-1400.webp"></source> <img src="/assets/rl/img/DifferentExpDQN.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="exploration rates" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-2"> </div> </div> <div class="caption"> Figure 2: Deep Q-Network Exploration Rate </div> <p>With a set exploration rate, models with different architectures were also trained (Figure 3). Modifications to the architecture were made because the scenario involves not only finding a path to the ball, but also interacting with it. Thus, more layers were added to represent the additional complexity. More neurons were added to the layers to better represent the states, but no significant performance boost was observed. The architecture that achieved the highest reward was chosen for the algorithm comparison.</p> <div class="row"> <div class="col-2"> </div> <div class="col-8"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/rl/img/DQN_all-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/rl/img/DQN_all-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/rl/img/DQN_all-1400.webp"></source> <img src="/assets/rl/img/DQN_all.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="dqn architectures" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-2"> </div> </div> <div class="caption"> Figure 3: Deep Q-Network Model Architecture Comparison </div> <p>The scenario was attempted multiple times by the Q-learning algorithm, DQN, and behavior-based algorithm developed from the game behavior module. The average time and standard deviation of completion time is summarized in the table below.</p> <div class="row"> <div class="col-2"> </div> <div class="col-8"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/rl/img/comparison_of_algorithms-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/rl/img/comparison_of_algorithms-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/rl/img/comparison_of_algorithms-1400.webp"></source> <img src="/assets/rl/img/comparison_of_algorithms.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="comparison of algorithms" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-2"> </div> </div> <p>The two reinforcement learning algorithms performed similarly in terms of mean time and standard deviation, but did not do better than the behavior-based algorithm. The behavior-based algorithm achieved both the best performance (low mean completion time) and consistency (low standard deviation). It is clear that the behavior-based algorithm has proven to be the best solution to the baseline scenario. Some reasons why reinforcement learning may not have performed as well as other systems may be attributed to the fact that the network architecture may not have been deep nor wide enough to properly express the complexity of the test scenario. Most deep reinforcement learning applications in literature use either very deep or wide architecture, neither of which were done. Instead, the number of layers were restricted to just three layers with 64 units each in order to quickly iterate through more architectures.</p> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="haarnoja2023learning" class="col-sm-8"> <div class="title">Learning agile soccer skills for a bipedal robot with deep reinforcement learning</div> <div class="author"> Tuomas Haarnoja, Ben Moran, Guy Lever, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Sandy H Huang, Dhruva Tirumala, Markus Wulfmeier, Jan Humplik, Saran Tunyasuvunakool, Noah Y Siegel, Roland Hafner, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2304.13653</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="ahn2023development" class="col-sm-8"> <div class="title">Development and Real-Time Optimization-based Control of a Full-sized Humanoid for Dynamic Walking and Running</div> <div class="author"> Min Sung Ahn</div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="zhu2021deep" class="col-sm-8"> <div class="title">Deep reinforcement learning based mobile robot navigation: A review</div> <div class="author"> Kai Zhu, and Tao Zhang</div> <div class="periodical"> <em>Tsinghua Science and Technology</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="ibarz2021train" class="col-sm-8"> <div class="title">How to train your robot with deep reinforcement learning: lessons we have learned</div> <div class="author"> Julian Ibarz, Jie Tan, Chelsea Finn, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Mrinal Kalakrishnan, Peter Pastor, Sergey Levine' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>The International Journal of Robotics Research</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Authors15" class="col-sm-8"> <div class="title">“Robocup Federation Official Website.” RoboCup Federation Official Website, 26 Feb. 2023, https://www.robocup.org/.</div> <div class="author"> Claude Sammut</div> <div class="periodical"> 2020 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="florensa2018automatic" class="col-sm-8"> <div class="title">Automatic goal generation for reinforcement learning agents</div> <div class="author"> Carlos Florensa, David Held, Xinyang Geng, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Pieter Abbeel' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In International conference on machine learning</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="riedmiller2018learning" class="col-sm-8"> <div class="title">Learning by playing solving sparse reward tasks from scratch</div> <div class="author"> Martin Riedmiller, Roland Hafner, Thomas Lampe, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Michael Neunert, Jonas Degrave, Tom Wiele, Vlad Mnih, Nicolas Heess, Jost Tobias Springenberg' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>In International conference on machine learning</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="pathak2017curiosity" class="col-sm-8"> <div class="title">Curiosity-driven exploration by self-supervised prediction</div> <div class="author"> Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Trevor Darrell' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In International conference on machine learning</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li></ol> <h2 class="bibliography">2013</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Authors14" class="col-sm-8"> <div class="title">Playing Atari with Deep Reinforcement Learning</div> <div class="author"> Koray Kavukcuoglu Volodymyr Mnih</div> <div class="periodical"> 2013 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li></ol> <h2 class="bibliography">2003</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="carreras2003proposal" class="col-sm-8"> <div class="title">A proposal of a behavior-based control architecture with reinforcement learning for an autonomous underwater robot</div> <div class="author"> Marc Carreras Pérez, and  others</div> <div class="periodical"> 2003 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li></ol> <h2 class="bibliography">1996</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="kaelbling1996reinforcement" class="col-sm-8"> <div class="title">Reinforcement learning: A survey</div> <div class="author"> Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore</div> <div class="periodical"> <em>Journal of artificial intelligence research</em>, 1996 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Ji Sung Ahn. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>